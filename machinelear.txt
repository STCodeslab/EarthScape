Machine Learning Models:
Develop machine learning models for predictive analysis of climate trends and impacts.
Include algorithms for anomaly detection, trend prediction, and correlation analysis.
Regularly update and refine models based on the latest available data.

now my next reqirement lets do it step  by step
ChatGPT said:
Thought for 1m 9s

Awesome ‚Äî let‚Äôs start Step 1: Anomaly Detection (baseline, production-ready).
This gives you an ML pipeline that (a) trains on your batch history, and (b) can score new/live points so you can plug it into your SSE stream.

Step 1 ‚Äî Anomaly Detection
What you‚Äôll get

/api/ml/anomaly/train ‚Äî trains an unsupervised model (Isolation Forest) on a dataset + field.

/api/ml/anomaly/score ‚Äî scores the latest N points (or a single new value) for anomalies.

/api/ml/models ‚Äî list saved models and their metadata.

Model artifacts saved as .pkl + metadata in Mongo (models collection).

Works with your existing db.datasets structure and JWT auth.

Backend: ml_blueprint.py

Drop this file next to your other blueprints and register it (e.g., app.register_blueprint(ml_bp, url_prefix="/api/ml")).

# ml_blueprint.py
from flask import Blueprint, request, jsonify
from bson import ObjectId
import os, io, json, datetime as dt
import numpy as np
import pandas as pd
from sklearn.ensemble import IsolationForest
import joblib

# Reuse your existing app globals
from app import db, JWT_SECRET, JWT_ALGO  # adjust import path if different
import jwt

ml_bp = Blueprint("ml_bp", __name__)
MODELS_DIR = os.path.join(os.getcwd(), "models")
os.makedirs(MODELS_DIR, exist_ok=True)

def _auth_ok():
    auth_header = request.headers.get("Authorization")
    if not auth_header:
        return (None, None, ("Authorization header missing", 401))
    try:
        token = auth_header.split(" ")[1]
        decoded = jwt.decode(token, JWT_SECRET, algorithms=[JWT_ALGO])
        user_id = decoded.get("user_id")
        role = decoded.get("role")
        if not user_id or not role:
            return (None, None, ("Invalid token payload", 401))
        return (user_id, role, None)
    except jwt.ExpiredSignatureError:
        return (None, None, ("Token expired", 401))
    except jwt.InvalidTokenError:
        return (None, None, ("Invalid token", 401))
    except Exception as e:
        return (None, None, (str(e), 401))

def _guess_time_column(df):
    candidates = ["timestamp", "time", "datetime", "date", "uploaded_at"]
    for c in candidates:
        if c in df.columns:
            return c
    return None

def _load_timeseries_from_mongo(user_id, role, dataset_name=None, dataset_id=None, target_field=None):
    """
    Loads records list from your db.datasets collection and returns a tidy DataFrame:
    columns: [ts, value] where ts is datetime64[ns], value is float
    """
    match_stage = {} if role == "admin" else {"user_id": ObjectId(user_id)}
    pipe = [{"$match": match_stage}, {"$unwind": "$datasets"}]
    if dataset_id:
        pipe.append({"$match": {"datasets._id": ObjectId(dataset_id)}})
    elif dataset_name:
        pipe.append({"$match": {"datasets.dataset_name": dataset_name}})

    pipe.append({"$project": {
        "records": "$datasets.records",
        "uploaded_at": "$datasets.uploaded_at",
        "dataset_name": "$datasets.dataset_name"
    }})

    docs = list(db.datasets.aggregate(pipe))
    if not docs:
        raise ValueError("Dataset not found or no access")

    # Combine all records across matches; typical case is 1
    all_records = []
    for d in docs:
        recs = d.get("records") or []
        # ensure it's a list of dicts
        if isinstance(recs, list):
            all_records.extend(recs)

    if not all_records:
        raise ValueError("No records found in the selected dataset")

    df = pd.DataFrame(all_records)
    if df.empty:
        raise ValueError("Records are empty")

    if target_field not in df.columns:
        raise ValueError(f"target_field '{target_field}' not present in dataset")

    # choose time column
    tcol = _guess_time_column(df)
    if tcol is None:
        # fallback: use index order if no timestamp; still construct a pseudo time
        df["_pseudo_time"] = pd.date_range(end=pd.Timestamp.utcnow(), periods=len(df), freq="min")
        tcol = "_pseudo_time"

    # coerce types
    df[tcol] = pd.to_datetime(df[tcol], errors="coerce", utc=True)
    df = df.dropna(subset=[tcol])
    df = df.sort_values(tcol)

    # numeric target
    df[target_field] = pd.to_numeric(df[target_field], errors="coerce")
    df = df.dropna(subset=[target_field])

    # return tidy
    return pd.DataFrame({"ts": df[tcol].values, "value": df[target_field].values})

def _make_features(df, window=5):
    """
    Create simple, robust features for IsolationForest:
    value, 1-lag diff, rolling mean, rolling std (filled)
    """
    s = pd.Series(df["value"])
    lag1 = s.diff().fillna(0.0)
    roll_mean = s.rolling(window, min_periods=1).mean()
    roll_std = s.rolling(window, min_periods=1).std().fillna(0.0)
    X = np.column_stack([s.values, lag1.values, roll_mean.values, roll_std.values])
    return X

def _save_model_artifact(model, meta):
    model_id = str(ObjectId())
    path = os.path.join(MODELS_DIR, f"{model_id}.pkl")
    joblib.dump({"model": model, "meta": meta}, path)
    meta_doc = {
        "_id": ObjectId(model_id),
        "type": "anomaly_isoforest",
        "created_at": dt.datetime.utcnow(),
        "path": path,
        **meta
    }
    db.models.insert_one(meta_doc)
    return model_id

def _load_model_artifact(model_id, ensure_type="anomaly_isoforest"):
    doc = db.models.find_one({"_id": ObjectId(model_id)})
    if not doc:
        raise ValueError("Model not found")
    if ensure_type and doc.get("type") != ensure_type:
        raise ValueError(f"Model type mismatch: expected {ensure_type}, got {doc.get('type')}")
    blob = joblib.load(doc["path"])
    return blob["model"], blob["meta"], doc

@ml_bp.route("/models", methods=["GET"])
def list_models():
    user_id, role, err = _auth_ok()
    if err: return jsonify({"error": err[0]}), err[1]

    q = {} if role == "admin" else {"user_id": ObjectId(user_id)}
    cur = db.models.find(q).sort("created_at", -1)
    out = []
    for d in cur:
        out.append({
            "model_id": str(d["_id"]),
            "type": d.get("type"),
            "dataset_name": d.get("dataset_name"),
            "target_field": d.get("target_field"),
            "created_at": d.get("created_at").isoformat() if d.get("created_at") else None,
            "metrics": d.get("metrics"),
        })
    return jsonify({"models": out}), 200

@ml_bp.route("/anomaly/train", methods=["POST"])
def anomaly_train():
    """
    JSON body:
    {
      "dataset_name": "karachi_weather",
      "dataset_id": "optional",
      "target_field": "temperature",
      "contamination": 0.02  // optional
    }
    """
    user_id, role, err = _auth_ok()
    if err: return jsonify({"error": err[0]}), err[1]

    payload = request.get_json(force=True)
    dataset_name = payload.get("dataset_name")
    dataset_id = payload.get("dataset_id")
    target_field = payload.get("target_field")
    contamination = float(payload.get("contamination", 0.02))

    if not target_field or (not dataset_name and not dataset_id):
        return jsonify({"error": "target_field and (dataset_name or dataset_id) are required"}), 400

    try:
        tsdf = _load_timeseries_from_mongo(user_id, role, dataset_name, dataset_id, target_field)
        if len(tsdf) < 20:
            return jsonify({"error": "Not enough data to train (min 20 rows)"}), 400

        X = _make_features(tsdf)
        model = IsolationForest(n_estimators=200, contamination=contamination, random_state=42)
        model.fit(X)

        # score training set for basic metrics
        raw_scores = model.decision_function(X)  # higher is more normal
        preds = model.predict(X)                 # 1 normal, -1 anomaly
        anomaly_rate = float((preds == -1).mean())

        meta = {
            "user_id": ObjectId(user_id),
            "role": role,
            "dataset_name": dataset_name,
            "dataset_id": ObjectId(dataset_id) if dataset_id else None,
            "target_field": target_field,
            "contamination": contamination,
            "feature_window": 5,
            "metrics": {
                "train_rows": int(len(X)),
                "anomaly_rate": anomaly_rate,
                "score_min": float(np.min(raw_scores)),
                "score_max": float(np.max(raw_scores)),
                "score_mean": float(np.mean(raw_scores)),
            }
        }

        model_id = _save_model_artifact(model, meta)

        return jsonify({
            "model_id": model_id,
            "type": "anomaly_isoforest",
            "metrics": meta["metrics"]
        }), 200

    except Exception as e:
        return jsonify({"error": str(e)}), 500

@ml_bp.route("/anomaly/score", methods=["POST"])
def anomaly_score():
    """
    Score latest points or an incoming single point.

    Body options:
    1) Score latest N points from the original dataset:
       { "model_id": "...", "latest_n": 50 }

    2) Score a single new value (real-time):
       { "model_id": "...", "value": 37.4 }
       (Server rebuilds small context window from history for features)
    """
    user_id, role, err = _auth_ok()
    if err: return jsonify({"error": err[0]}), err[1]

    payload = request.get_json(force=True)
    model_id = payload.get("model_id")
    latest_n = payload.get("latest_n")
    new_value = payload.get("value")

    if not model_id:
        return jsonify({"error": "model_id is required"}), 400

    try:
        model, meta, model_doc = _load_model_artifact(model_id)

        # permissions: model owner or admin
        if role != "admin" and str(model_doc.get("user_id")) != str(user_id):
            return jsonify({"error": "Forbidden"}), 403

        # Pull history to build features window
        tsdf = _load_timeseries_from_mongo(
            user_id=user_id if role != "admin" else str(model_doc.get("user_id")),
            role=role if role != "admin" else "admin",
            dataset_name=meta.get("dataset_name"),
            dataset_id=str(meta.get("dataset_id")) if meta.get("dataset_id") else None,
            target_field=meta["target_field"]
        )

        if new_value is not None:
            # append the new value with current timestamp
            tsdf = pd.concat([tsdf, pd.DataFrame({"ts": [pd.Timestamp.utcnow()], "value": [float(new_value)]})], ignore_index=True)

        if latest_n:
            tsdf = tsdf.tail(int(latest_n))

        X = _make_features(tsdf)
        scores = model.decision_function(X)  # higher is more normal
        preds = model.predict(X)             # 1 normal, -1 anomaly

        out = []
        for i, row in tsdf.reset_index(drop=True).iterrows():
            out.append({
                "ts": pd.to_datetime(row["ts"]).isoformat(),
                "value": float(row["value"]),
                "score": float(scores[i]),
                "is_anomaly": bool(preds[i] == -1)
            })

        return jsonify({"results": out[-(int(latest_n) if latest_n else len(out)) : ]}), 200

    except Exception as e:
        return jsonify({"error": str(e)}), 500

Register the blueprint
# in your app factory / main app file
from ml_blueprint import ml_bp
app.register_blueprint(ml_bp, url_prefix="/api/ml")

How to use (quick)

Train

POST /api/ml/anomaly/train
Authorization: Bearer <token>
{
  "dataset_name": "karachi_weather",
  "target_field": "temperature",
  "contamination": 0.02
}


Score last 50

POST /api/ml/anomaly/score
Authorization: Bearer <token>
{
  "model_id": "<returned_from_train>",
  "latest_n": 50
}


Score a single live value (use from your SSE publisher)

POST /api/ml/anomaly/score
Authorization: Bearer <token>
{
  "model_id": "<returned_from_train>",
  "value": 41.7
}


The response includes is_anomaly: true/false plus a continuous score you can threshold for alerts.

Where this fits your real-time + batch

Batch: anomaly/train learns from your historical dataset (db.datasets.datasets.records).

Real-time: Whenever a new record arrives (your existing pipeline that emits to /api/stream/datasets), call anomaly/score with the latest value to instantly detect anomalies and surface alerts in UI (toast or a dedicated Anomaly Alerts card).

Next step

Step 2: Trend Prediction (forecasting).
I‚Äôll add a forecasting module (lag-feature regression or ARIMA) with endpoints:

POST /api/ml/forecast/train

POST /api/ml/forecast/predict (multi-step horizon)

Rolling re-train to ‚Äúregularly update and refine models‚Äù.

Say ‚Äúnext‚Äù and I‚Äôll drop the forecasting code.










Awesome ‚Äî Step 2: Trend Prediction (forecasting) coming right up.
We‚Äôll add two endpoints that fit historical data and generate multi-step forecasts you can plot alongside your live feed.

What you get

POST /api/ml/forecast/train ‚Üí fits an ARIMA model (auto-selects (p,d,q) via small grid using AIC).

POST /api/ml/forecast/predict ‚Üí predicts next horizon steps with confidence intervals.

Uses the same model store as Step 1 (artifacts in models/ + metadata in db.models).

Works with your db.datasets.datasets.records structure and JWT.

Add to your existing ml_blueprint.py

These endpoints assume the helpers you already have from Step 1: _auth_ok, _load_timeseries_from_mongo, _save_model_artifact, _load_model_artifact.

# --- imports at top of file (add if missing) ---
from statsmodels.tsa.arima.model import ARIMA
import warnings
warnings.filterwarnings("ignore")

# ========= STEP 2: FORECASTING =========

def _infer_freq(ts: pd.Series):
    # Try pandas to infer; fallback to median diff in minutes
    freq = pd.infer_freq(ts)
    if freq is None and len(ts) >= 3:
        diffs = pd.Series(ts.sort_values().diff().dropna().values).dt.total_seconds()
        if not diffs.empty:
            med = diffs.median()
            # map common seconds to pandas freq strings
            mapping = {
                60: "T",           # 1 minute
                300: "5T",
                900: "15T",
                1800: "30T",
                3600: "H",
                86400: "D"
            }
            # find closest
            closest = min(mapping.keys(), key=lambda x: abs(x - med))
            freq = mapping[closest]
    return freq  # may still be None; ARIMA can handle but forecasting index will be range

def _to_timeseries(tsdf: pd.DataFrame):
    """Ensure a proper time-indexed series returned as (y, index, freq)."""
    s = pd.Series(tsdf["value"].astype(float).values, index=pd.to_datetime(tsdf["ts"], utc=True))
    s = s.sort_index()
    freq = _infer_freq(s.index.to_series())
    if freq:
        s = s.asfreq(freq)
        # fill small gaps simply; advanced imputation could be added later
        s = s.interpolate(limit_direction="both")
    return s, freq

@ml_bp.route("/forecast/train", methods=["POST"])
def forecast_train():
    """
    Train ARIMA on a dataset/time-series field.

    Body:
    {
      "dataset_name": "karachi_weather",  // or "dataset_id": "..."
      "target_field": "temperature",
      "p_range": [0, 1, 2],               // optional small grid
      "d_range": [0, 1, 2],               // optional
      "q_range": [0, 1, 2]                // optional
    }
    """
    user_id, role, err = _auth_ok()
    if err: return jsonify({"error": err[0]}), err[1]

    body = request.get_json(force=True)
    dataset_name = body.get("dataset_name")
    dataset_id = body.get("dataset_id")
    target_field = body.get("target_field")
    p_range = body.get("p_range", [0, 1, 2])
    d_range = body.get("d_range", [0, 1, 2])
    q_range = body.get("q_range", [0, 1, 2])

    if not target_field or (not dataset_name and not dataset_id):
        return jsonify({"error": "target_field and (dataset_name or dataset_id) are required"}), 400

    try:
        tsdf = _load_timeseries_from_mongo(user_id, role, dataset_name, dataset_id, target_field)
        if len(tsdf) < 20:
            return jsonify({"error": "Not enough data to train (min 20 rows)"}), 400

        y, freq = _to_timeseries(tsdf)
        if y.isna().all():
            return jsonify({"error": "Series is all NaN after preprocessing"}), 400

        # Small grid search for (p,d,q) by AIC
        best = {"aic": float("inf"), "order": None, "model": None}
        for p in p_range:
            for d in d_range:
                for q in q_range:
                    try:
                        m = ARIMA(y, order=(p, d, q))
                        res = m.fit()
                        if res.aic < best["aic"]:
                            best = {"aic": res.aic, "order": (p, d, q), "model": res}
                    except Exception:
                        continue

        if best["model"] is None:
            return jsonify({"error": "Failed to fit ARIMA on any (p,d,q) in the grid"}), 500

        res = best["model"]
        order = best["order"]

        meta = {
            "user_id": ObjectId(user_id),
            "role": role,
            "type": "forecast_arima",
            "dataset_name": dataset_name,
            "dataset_id": ObjectId(dataset_id) if dataset_id else None,
            "target_field": target_field,
            "order": {"p": order[0], "d": order[1], "q": order[2]},
            "freq": freq,
            "metrics": {
                "aic": float(best["aic"]),
                "train_rows": int(len(y)),
            }
        }

        # Save fitted results (statsmodels results object)
        model_id = _save_model_artifact(res, meta)  # stores results + meta

        return jsonify({
            "model_id": model_id,
            "type": "forecast_arima",
            "order": meta["order"],
            "freq": meta["freq"],
            "metrics": meta["metrics"]
        }), 200

    except Exception as e:
        return jsonify({"error": str(e)}), 500

@ml_bp.route("/forecast/predict", methods=["POST"])
def forecast_predict():
    """
    Predict next horizon steps.

    Body:
    {
      "model_id": "...",
      "horizon": 24               // steps ahead
    }
    """
    user_id, role, err = _auth_ok()
    if err: return jsonify({"error": err[0]}), err[1]

    body = request.get_json(force=True)
    model_id = body.get("model_id")
    horizon = int(body.get("horizon", 24))
    if not model_id:
        return jsonify({"error": "model_id is required"}), 400

    try:
        model, meta, model_doc = _load_model_artifact(model_id, ensure_type="forecast_arima")

        # permissions: owner or admin
        if role != "admin" and str(model_doc.get("user_id")) != str(user_id):
            return jsonify({"error": "Forbidden"}), 403

        # Rebuild latest series to forecast from up-to-date data
        tsdf = _load_timeseries_from_mongo(
            user_id=user_id if role != "admin" else str(model_doc.get("user_id")),
            role=role if role != "admin" else "admin",
            dataset_name=meta.get("dataset_name"),
            dataset_id=str(meta.get("dataset_id")) if meta.get("dataset_id") else None,
            target_field=meta["target_field"]
        )
        y, freq = _to_timeseries(tsdf)

        # If the stored model was trained earlier, refit to the full latest y using same order
        order = (meta["order"]["p"], meta["order"]["d"], meta["order"]["q"])
        refit = ARIMA(y, order=order).fit()

        forecast_res = refit.get_forecast(steps=horizon)
        mean = forecast_res.predicted_mean
        conf = forecast_res.conf_int(alpha=0.05)  # 95% CI

        # Build timestamps for forecast horizon
        if freq:
            idx = pd.date_range(start=y.index[-1] + pd.tseries.frequencies.to_offset(freq),
                                periods=horizon, freq=freq)
        else:
            # Fallback to simple integer steps if freq unknown
            idx = range(1, horizon + 1)

        out = []
        for i in range(horizon):
            ts_val = (idx[i].isoformat() if hasattr(idx[i], "isoformat") else int(idx[i]))
            out.append({
                "ts": ts_val,
                "yhat": float(mean.iloc[i]),
                "yhat_lower": float(conf.iloc[i, 0]),
                "yhat_upper": float(conf.iloc[i, 1])
            })

        return jsonify({
            "model_id": model_id,
            "target_field": meta["target_field"],
            "order": meta["order"],
            "freq": freq or meta.get("freq"),
            "forecast": out
        }), 200

    except Exception as e:
        return jsonify({"error": str(e)}), 500

Quick usage

Train

POST /api/ml/forecast/train
Authorization: Bearer <token>
{
  "dataset_name": "karachi_weather",
  "target_field": "temperature"
}


Predict 48 steps ahead

POST /api/ml/forecast/predict
Authorization: Bearer <token>
{
  "model_id": "<returned_from_train>",
  "horizon": 48
}


Response snippet

{
  "forecast": [
    { "ts": "2025-08-26T18:30:00+00:00", "yhat": 34.2, "yhat_lower": 31.0, "yhat_upper": 37.5 },
    ...
  ]
}

How to wire it into your UI quickly

Call /forecast/predict on page load (and periodically), then draw a forecast band on your existing chart:

Actuals: solid line.

Forecast: dashed line.

Confidence interval: shaded area between yhat_lower and yhat_upper.

‚ÄúRegularly update and refine‚Äù

Add a cron/worker (Celery/CRON) to hit /forecast/train nightly per dataset/field.

Keep latest N models or overwrite by dataset_name + target_field.

Log AIC; alert if it worsens significantly ‚Üí auto-expand grid ranges.

Say ‚Äúnext‚Äù and I‚Äôll deliver Step 3: Correlation Analysis (auto-correlation, cross-dataset correlation matrix + endpoints, and a small UI card to explore relationships).





































# ml_blueprint.py
from flask import Blueprint, request, jsonify
from bson import ObjectId
import os, io, json, datetime as dt
import numpy as np
import pandas as pd
from sklearn.ensemble import IsolationForest
import joblib
from statsmodels.tsa.arima.model import ARIMA
import warnings
warnings.filterwarnings("ignore")
# Reuse your existing app globals
from db import db
import jwt
JWT_SECRET = os.getenv("JWT_SECRET", "supersecretkey")
JWT_ALGO = os.getenv("JWT_ALGORITHM", "HS256")

ml_bp = Blueprint("ml_bp", __name__)
MODELS_DIR = os.path.join(os.getcwd(), "models")
os.makedirs(MODELS_DIR, exist_ok=True)

def _auth_ok():
    auth_header = request.headers.get("Authorization")
    if not auth_header:
        return (None, None, ("Authorization header missing", 401))
    try:
        token = auth_header.split(" ")[1]
        decoded = jwt.decode(token, JWT_SECRET, algorithms=[JWT_ALGO])
        user_id = decoded.get("user_id")
        role = decoded.get("role")
        if not user_id or not role:
            return (None, None, ("Invalid token payload", 401))
        return (user_id, role, None)
    except jwt.ExpiredSignatureError:
        return (None, None, ("Token expired", 401))
    except jwt.InvalidTokenError:
        return (None, None, ("Invalid token", 401))
    except Exception as e:
        return (None, None, (str(e), 401))

def _guess_time_column(df):
    candidates = ["timestamp", "time", "datetime", "date", "uploaded_at"]
    for c in candidates:
        if c in df.columns:
            return c
    return None

def _load_timeseries_from_mongo(user_id, role, dataset_name=None, dataset_id=None, target_field=None):
    """
    Loads records list from your db.datasets collection and returns a tidy DataFrame:
    columns: [ts, value] where ts is datetime64[ns], value is float
    """
    match_stage = {} if role == "admin" else {"user_id": ObjectId(user_id)}
    pipe = [{"$match": match_stage}, {"$unwind": "$datasets"}]
    if dataset_id:
        pipe.append({"$match": {"datasets._id": ObjectId(dataset_id)}})
    elif dataset_name:
        pipe.append({"$match": {"datasets.dataset_name": dataset_name}})

    pipe.append({"$project": {
        "records": "$datasets.records",
        "uploaded_at": "$datasets.uploaded_at",
        "dataset_name": "$datasets.dataset_name"
    }})

    docs = list(db.datasets.aggregate(pipe))
    if not docs:
        raise ValueError("Dataset not found or no access")

    # Combine all records across matches; typical case is 1
    all_records = []
    for d in docs:
        recs = d.get("records") or []
        # ensure it's a list of dicts
        if isinstance(recs, list):
            all_records.extend(recs)

    if not all_records:
        raise ValueError("No records found in the selected dataset")

    df = pd.DataFrame(all_records)
    if df.empty:
        raise ValueError("Records are empty")

    if target_field not in df.columns:
        raise ValueError(f"target_field '{target_field}' not present in dataset")

    # choose time column
    tcol = _guess_time_column(df)
    if tcol is None:
        # fallback: use index order if no timestamp; still construct a pseudo time
        df["_pseudo_time"] = pd.date_range(end=pd.Timestamp.utcnow(), periods=len(df), freq="min")
        tcol = "_pseudo_time"

    # coerce types
    df[tcol] = pd.to_datetime(df[tcol], errors="coerce", utc=True)
    df = df.dropna(subset=[tcol])
    df = df.sort_values(tcol)

    # numeric target
    df[target_field] = pd.to_numeric(df[target_field], errors="coerce")
    df = df.dropna(subset=[target_field])

    # return tidy
    return pd.DataFrame({"ts": df[tcol].values, "value": df[target_field].values})

def _make_features(df, window=5):
    """
    Create simple, robust features for IsolationForest:
    value, 1-lag diff, rolling mean, rolling std (filled)
    """
    s = pd.Series(df["value"])
    lag1 = s.diff().fillna(0.0)
    roll_mean = s.rolling(window, min_periods=1).mean()
    roll_std = s.rolling(window, min_periods=1).std().fillna(0.0)
    X = np.column_stack([s.values, lag1.values, roll_mean.values, roll_std.values])
    return X

def _save_model_artifact(model, meta):
    model_id = str(ObjectId())
    path = os.path.join(MODELS_DIR, f"{model_id}.pkl")
    joblib.dump({"model": model, "meta": meta}, path)
    meta_doc = {
        "_id": ObjectId(model_id),
        "type": "anomaly_isoforest",
        "created_at": dt.datetime.utcnow(),
        "path": path,
        **meta
    }
    db.models.insert_one(meta_doc)
    return model_id

def _load_model_artifact(model_id, ensure_type="anomaly_isoforest"):
    doc = db.models.find_one({"_id": ObjectId(model_id)})
    if not doc:
        raise ValueError("Model not found")
    if ensure_type and doc.get("type") != ensure_type:
        raise ValueError(f"Model type mismatch: expected {ensure_type}, got {doc.get('type')}")
    blob = joblib.load(doc["path"])
    return blob["model"], blob["meta"], doc

@ml_bp.route("/models", methods=["GET"])
def list_models():
    user_id, role, err = _auth_ok()
    if err: return jsonify({"error": err[0]}), err[1]

    q = {} if role == "admin" else {"user_id": ObjectId(user_id)}
    cur = db.models.find(q).sort("created_at", -1)
    out = []
    for d in cur:
        out.append({
            "model_id": str(d["_id"]),
            "type": d.get("type"),
            "dataset_name": d.get("dataset_name"),
            "target_field": d.get("target_field"),
            "created_at": d.get("created_at").isoformat() if d.get("created_at") else None,
            "metrics": d.get("metrics"),
        })
    return jsonify({"models": out}), 200

@ml_bp.route("/anomaly/train", methods=["POST"])
def anomaly_train():
    """
    JSON body:
    {
      "dataset_name": "karachi_weather",
      "dataset_id": "optional",
      "target_field": "temperature",
      "contamination": 0.02  // optional
    }
    """
    user_id, role, err = _auth_ok()
    if err: return jsonify({"error": err[0]}), err[1]

    payload = request.get_json(force=True)
    dataset_name = payload.get("dataset_name")
    dataset_id = payload.get("dataset_id")
    target_field = payload.get("target_field")
    contamination = float(payload.get("contamination", 0.02))

    if not target_field or (not dataset_name and not dataset_id):
        return jsonify({"error": "target_field and (dataset_name or dataset_id) are required"}), 400

    try:
        tsdf = _load_timeseries_from_mongo(user_id, role, dataset_name, dataset_id, target_field)
        if len(tsdf) < 20:
            return jsonify({"error": "Not enough data to train (min 20 rows)"}), 400

        X = _make_features(tsdf)
        model = IsolationForest(n_estimators=200, contamination=contamination, random_state=42)
        model.fit(X)

        # score training set for basic metrics
        raw_scores = model.decision_function(X)  # higher is more normal
        preds = model.predict(X)                 # 1 normal, -1 anomaly
        anomaly_rate = float((preds == -1).mean())

        meta = {
            "user_id": ObjectId(user_id),
            "role": role,
            "dataset_name": dataset_name,
            "dataset_id": ObjectId(dataset_id) if dataset_id else None,
            "target_field": target_field,
            "contamination": contamination,
            "feature_window": 5,
            "metrics": {
                "train_rows": int(len(X)),
                "anomaly_rate": anomaly_rate,
                "score_min": float(np.min(raw_scores)),
                "score_max": float(np.max(raw_scores)),
                "score_mean": float(np.mean(raw_scores)),
            }
        }

        model_id = _save_model_artifact(model, meta)

        return jsonify({
            "model_id": model_id,
            "type": "anomaly_isoforest",
            "metrics": meta["metrics"]
        }), 200

    except Exception as e:
        return jsonify({"error": str(e)}), 500

@ml_bp.route("/anomaly/score", methods=["POST"])
def anomaly_score():
    """
    Score latest points or an incoming single point.

    Body options:
    1) Score latest N points from the original dataset:
       { "model_id": "...", "latest_n": 50 }

    2) Score a single new value (real-time):
       { "model_id": "...", "value": 37.4 }
       (Server rebuilds small context window from history for features)
    """
    user_id, role, err = _auth_ok()
    if err: return jsonify({"error": err[0]}), err[1]

    payload = request.get_json(force=True)
    model_id = payload.get("model_id")
    latest_n = payload.get("latest_n")
    new_value = payload.get("value")

    if not model_id:
        return jsonify({"error": "model_id is required"}), 400

    try:
        model, meta, model_doc = _load_model_artifact(model_id)

        # permissions: model owner or admin
        if role != "admin" and str(model_doc.get("user_id")) != str(user_id):
            return jsonify({"error": "Forbidden"}), 403

        # Pull history to build features window
        tsdf = _load_timeseries_from_mongo(
            user_id=user_id if role != "admin" else str(model_doc.get("user_id")),
            role=role if role != "admin" else "admin",
            dataset_name=meta.get("dataset_name"),
            dataset_id=str(meta.get("dataset_id")) if meta.get("dataset_id") else None,
            target_field=meta["target_field"]
        )

        if new_value is not None:
            # append the new value with current timestamp
            tsdf = pd.concat([tsdf, pd.DataFrame({"ts": [pd.Timestamp.utcnow()], "value": [float(new_value)]})], ignore_index=True)

        if latest_n:
            tsdf = tsdf.tail(int(latest_n))

        X = _make_features(tsdf)
        scores = model.decision_function(X)  # higher is more normal
        preds = model.predict(X)             # 1 normal, -1 anomaly

        out = []
        for i, row in tsdf.reset_index(drop=True).iterrows():
            out.append({
                "ts": pd.to_datetime(row["ts"]).isoformat(),
                "value": float(row["value"]),
                "score": float(scores[i]),
                "is_anomaly": bool(preds[i] == -1)
            })

        return jsonify({"results": out[-(int(latest_n) if latest_n else len(out)) : ]}), 200

    except Exception as e:
        return jsonify({"error": str(e)}), 500


@ml_bp.route("/models/<model_id>", methods=["DELETE"])
def delete_model(model_id):
    user_id, role, err = _auth_ok()
    if err:
        return jsonify({"error": err[0]}), err[1]

    try:
        # Find the model
        doc = db.models.find_one({"_id": ObjectId(model_id)})
        if not doc:
            return jsonify({"error": "Model not found"}), 404

        # Permissions: only owner or admin
        if role != "admin" and str(doc.get("user_id")) != str(user_id):
            return jsonify({"error": "Forbidden"}), 403

        # Delete model file if exists
        path = doc.get("path")
        if path and os.path.exists(path):
            os.remove(path)

        # Delete from DB
        db.models.delete_one({"_id": ObjectId(model_id)})

        return jsonify({"message": f"Model {model_id} deleted successfully"}), 200

    except Exception as e:
        return jsonify({"error": str(e)}), 500




# ========= STEP 2: FORECASTING =========

def _infer_freq(ts: pd.Series):
    # Try pandas to infer; fallback to median diff in minutes
    freq = pd.infer_freq(ts)
    if freq is None and len(ts) >= 3:
        diffs = pd.Series(ts.sort_values().diff().dropna().values).dt.total_seconds()
        if not diffs.empty:
            med = diffs.median()
            # map common seconds to pandas freq strings
            mapping = {
                60: "T",           # 1 minute
                300: "5T",
                900: "15T",
                1800: "30T",
                3600: "H",
                86400: "D"
            }
            # find closest
            closest = min(mapping.keys(), key=lambda x: abs(x - med))
            freq = mapping[closest]
    return freq  # may still be None; ARIMA can handle but forecasting index will be range

def _to_timeseries(tsdf: pd.DataFrame):
    """Ensure a proper time-indexed series returned as (y, index, freq)."""
    s = pd.Series(tsdf["value"].astype(float).values, index=pd.to_datetime(tsdf["ts"], utc=True))
    s = s.sort_index()

    # üöÄ FIX: handle duplicate timestamps
    if s.index.has_duplicates:
        # Option 1: keep first occurrence
        s = s[~s.index.duplicated(keep="first")]
        # Option 2: OR aggregate duplicates by mean:
        # s = s.groupby(s.index).mean()

    freq = _infer_freq(s.index.to_series())
    if freq:
        s = s.asfreq(freq)
        s = s.interpolate(limit_direction="both")
    return s, freq


@ml_bp.route("/forecast/train", methods=["POST"])
def forecast_train():
    """
    Train ARIMA on a dataset/time-series field.

    Body:
    {
      "dataset_name": "karachi_weather",  // or "dataset_id": "..."
      "target_field": "temperature",
      "p_range": [0, 1, 2],               // optional small grid
      "d_range": [0, 1, 2],               // optional
      "q_range": [0, 1, 2]                // optional
    }
    """
    user_id, role, err = _auth_ok()
    if err: return jsonify({"error": err[0]}), err[1]

    body = request.get_json(force=True)
    dataset_name = body.get("dataset_name")
    dataset_id = body.get("dataset_id")
    target_field = body.get("target_field")
    p_range = body.get("p_range", [0, 1, 2])
    d_range = body.get("d_range", [0, 1, 2])
    q_range = body.get("q_range", [0, 1, 2])

    if not target_field or (not dataset_name and not dataset_id):
        return jsonify({"error": "target_field and (dataset_name or dataset_id) are required"}), 400

    try:
        tsdf = _load_timeseries_from_mongo(user_id, role, dataset_name, dataset_id, target_field)
        if len(tsdf) < 20:
            return jsonify({"error": "Not enough data to train (min 20 rows)"}), 400

        y, freq = _to_timeseries(tsdf)
        if y.isna().all():
            return jsonify({"error": "Series is all NaN after preprocessing"}), 400

        # Small grid search for (p,d,q) by AIC
        best = {"aic": float("inf"), "order": None, "model": None}
        for p in p_range:
            for d in d_range:
                for q in q_range:
                    try:
                        m = ARIMA(y, order=(p, d, q))
                        res = m.fit()
                        if res.aic < best["aic"]:
                            best = {"aic": res.aic, "order": (p, d, q), "model": res}
                    except Exception:
                        continue

        if best["model"] is None:
            return jsonify({"error": "Failed to fit ARIMA on any (p,d,q) in the grid"}), 500

        res = best["model"]
        order = best["order"]

        meta = {
            "user_id": ObjectId(user_id),
            "role": role,
            "type": "forecast_arima",
            "dataset_name": dataset_name,
            "dataset_id": ObjectId(dataset_id) if dataset_id else None,
            "target_field": target_field,
            "order": {"p": order[0], "d": order[1], "q": order[2]},
            "freq": freq,
            "metrics": {
                "aic": float(best["aic"]),
                "train_rows": int(len(y)),
            }
        }

        # Save fitted results (statsmodels results object)
        model_id = _save_model_artifact(res, meta)  # stores results + meta

        return jsonify({
            "model_id": model_id,
            "type": "forecast_arima",
            "order": meta["order"],
            "freq": meta["freq"],
            "metrics": meta["metrics"]
        }), 200

    except Exception as e:
        return jsonify({"error": str(e)}), 500

@ml_bp.route("/forecast/predict", methods=["POST"])
def forecast_predict():
    """
    Predict next horizon steps.

    Body:
    {
      "model_id": "...",
      "horizon": 24               // steps ahead
    }
    """
    user_id, role, err = _auth_ok()
    if err: return jsonify({"error": err[0]}), err[1]

    body = request.get_json(force=True)
    model_id = body.get("model_id")
    horizon = int(body.get("horizon", 24))
    if not model_id:
        return jsonify({"error": "model_id is required"}), 400

    try:
        model, meta, model_doc = _load_model_artifact(model_id, ensure_type="forecast_arima")

        # permissions: owner or admin
        if role != "admin" and str(model_doc.get("user_id")) != str(user_id):
            return jsonify({"error": "Forbidden"}), 403

        # Rebuild latest series to forecast from up-to-date data
        tsdf = _load_timeseries_from_mongo(
            user_id=user_id if role != "admin" else str(model_doc.get("user_id")),
            role=role if role != "admin" else "admin",
            dataset_name=meta.get("dataset_name"),
            dataset_id=str(meta.get("dataset_id")) if meta.get("dataset_id") else None,
            target_field=meta["target_field"]
        )
        y, freq = _to_timeseries(tsdf)

        # If the stored model was trained earlier, refit to the full latest y using same order
        order = (meta["order"]["p"], meta["order"]["d"], meta["order"]["q"])
        refit = ARIMA(y, order=order).fit()

        forecast_res = refit.get_forecast(steps=horizon)
        mean = forecast_res.predicted_mean
        conf = forecast_res.conf_int(alpha=0.05)  # 95% CI

        # Build timestamps for forecast horizon
        if freq:
            idx = pd.date_range(start=y.index[-1] + pd.tseries.frequencies.to_offset(freq),
                                periods=horizon, freq=freq)
        else:
            # Fallback to simple integer steps if freq unknown
            idx = range(1, horizon + 1)

        out = []
        for i in range(horizon):
            ts_val = (idx[i].isoformat() if hasattr(idx[i], "isoformat") else int(idx[i]))
            out.append({
                "ts": ts_val,
                "yhat": float(mean.iloc[i]),
                "yhat_lower": float(conf.iloc[i, 0]),
                "yhat_upper": float(conf.iloc[i, 1])
            })

        return jsonify({
            "model_id": model_id,
            "target_field": meta["target_field"],
            "order": meta["order"],
            "freq": freq or meta.get("freq"),
            "forecast": out
        }), 200

    except Exception as e:
        return jsonify({"error": str(e)}), 500
